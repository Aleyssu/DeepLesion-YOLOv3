{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo2H_L-yNpmX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from math import ceil\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import dill\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.collections import PatchCollection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eNUy1v2Dw_qn"
      },
      "source": [
        "**\"DL_info.csv\" and \"images/\" (contains all the folders with the dataset's images) should be in the same directory as the script before you run this**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf7_z4IPwfCw"
      },
      "source": [
        "Load and preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS0EPwQMMuj4"
      },
      "outputs": [],
      "source": [
        "CSV_DF_PATH = 'DL_info.csv'\n",
        "BASE_IMG_DIR = 'images'\n",
        "GRID_SIZE = 16  # 16x16 grid divides images into 32x32 pixel cells\n",
        "BOXES_PER_CELL = 1  # Number of bounding boxes per cell\n",
        "NUM_COORDS = 4  # Coordinates for each box: (x, y, width, height)\n",
        "IMAGE_SIZE = 512\n",
        "# Number of outputs per grid cell (e.g., 2 boxes * (4 coords + 1 confidence))\n",
        "OUTPUT_CHANNELS = BOXES_PER_CELL * (NUM_COORDS + 1)  # Just confidence, no class predictions here\n",
        "GRID_WIDTH = IMAGE_SIZE / GRID_SIZE\n",
        "\n",
        "# Returns a list with preprocessed dataset images and another for bounding box grids in YOLO format\n",
        "def generate_dataset():\n",
        "    # Image transformation pipeline\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),  # Convert to PyTorch tensor (scales to [0, 1])\n",
        "        transforms.Lambda(lambda x: x - 32768),  # Subtract 32768\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))  # Resize to desired dimensions\n",
        "    ])\n",
        "    csv_df = pd.read_csv(CSV_DF_PATH)\n",
        "    prev_entry_img_name = None\n",
        "    bounding_boxes = []\n",
        "    images = []\n",
        "    boxes = []\n",
        "    iterator = iter(csv_df.values)\n",
        "    img_names = []\n",
        "    for entry in iterator:\n",
        "        # debug_var = False\n",
        "        # For images with different dimensions\n",
        "        x_rescale_factor = 1\n",
        "        y_rescale_factor = 1\n",
        "        img_name = entry[0]\n",
        "\n",
        "        # Some images have multiple bounding boxes. Their entries in the csv are\n",
        "        # side-by-side so we can check if the previous entry had the same image\n",
        "        # and group their bounding boxes together if so\n",
        "        if img_name != prev_entry_img_name:\n",
        "            # Last bounding box in the image found; convert and store them in the tf dataset\n",
        "            if len(bounding_boxes) != 0: \n",
        "                # Initiallize empty YOLO grid\n",
        "                y_true = torch.zeros((GRID_SIZE, GRID_SIZE, OUTPUT_CHANNELS))\n",
        "                # Convert bounding boxes to YOLO grid format\n",
        "                for box in bounding_boxes:\n",
        "                    # Get index of grid where bounding box lies\n",
        "                    grid_idx_x = min(max(ceil(box[0] / GRID_WIDTH) - 1, 0), GRID_SIZE - 1)\n",
        "                    grid_idx_y = min(max(ceil(box[1] / GRID_WIDTH) - 1, 0), GRID_SIZE - 1)\n",
        "                    # Get coordinate of bounding box within grid\n",
        "                    box_x = box[0] / GRID_WIDTH - grid_idx_x\n",
        "                    box_y = box[1] / GRID_WIDTH - grid_idx_y\n",
        "                    # Get relative widths to the image size\n",
        "                    rw = box[2] / IMAGE_SIZE\n",
        "                    rh = box[3] / IMAGE_SIZE\n",
        "                    y_true[grid_idx_x, grid_idx_y, :] = torch.tensor([box_x, box_y, rw, rh, 1]) \n",
        "                # Convert grid to tensor and store in dataset list\n",
        "                boxes.append(y_true)\n",
        "                images.append(image)\n",
        "                img_names.append(prev_entry_img_name)\n",
        "\n",
        "                bounding_boxes = []\n",
        "\n",
        "            # Load and decode the image\n",
        "            img_path = os.path.join(BASE_IMG_DIR, img_name[:12], img_name[13:])\n",
        "            # Make sure chosen image exists before loading\n",
        "            while not os.path.exists(img_path):\n",
        "                entry = next(iterator, None)\n",
        "                if entry is None:\n",
        "                    return images, boxes, img_names\n",
        "                img_name = entry[0]\n",
        "                img_path = os.path.join(BASE_IMG_DIR, img_name[:12], img_name[13:])\n",
        "            with Image.open(img_path).convert(\"F\") as im:\n",
        "                original_size = im.size\n",
        "                image = transform(im)\n",
        "                # print(image)\n",
        "            if original_size != (IMAGE_SIZE, IMAGE_SIZE):\n",
        "                x_rescale_factor = IMAGE_SIZE / original_size[0]\n",
        "                y_rescale_factor = IMAGE_SIZE / original_size[1]\n",
        "                # Debug to track wrong dimension images\n",
        "                # plt.imsave(img_name, image[0], vmin = -1200, vmax = 600, cmap = 'gray')\n",
        "                # print(img_name)\n",
        "                # debug_var = True\n",
        "\n",
        "        # Format bounding boxes as list of floats and append to list\n",
        "        bbox = [float(x) for x in entry[6].split(\", \")]\n",
        "        # if debug_var:\n",
        "        #     print(bbox)\n",
        "        # Convert to [x_center, y_center, width, height] format for YOLO models\n",
        "        # Scale bounding box coordinates for images with different dimensions\n",
        "        width = (bbox[2] - bbox[0])\n",
        "        height = (bbox[3] - bbox[1])\n",
        "        x_center = (bbox[0] + width / 2) * x_rescale_factor\n",
        "        y_center = (bbox[1] + height / 2) * y_rescale_factor\n",
        "        width *= x_rescale_factor\n",
        "        height *= y_rescale_factor\n",
        "        bounding_boxes.append([x_center, y_center, width, height])\n",
        "\n",
        "        prev_entry_img_name = img_name\n",
        "    return images, boxes, img_names\n",
        "\n",
        "images, boxes, img_names = generate_dataset()\n",
        "images_tensor = torch.stack(images)\n",
        "boxes_tensor = torch.stack(boxes)\n",
        "print(images_tensor.shape)\n",
        "print(boxes_tensor.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform train-test-validation spllit and initialize dataloaders for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shuffle indices\n",
        "shuffle_indices = torch.randperm(images_tensor.size(0))\n",
        "\n",
        "# Shuffle features and labels\n",
        "shuffled_features = images_tensor[shuffle_indices]\n",
        "shuffled_labels = boxes_tensor[shuffle_indices]\n",
        "\n",
        "# Split ratio (75% test, 15% test, 10% validation)\n",
        "train_split = 0.75\n",
        "test_split = 0.90\n",
        "split_idx_train = int(images_tensor.size(0) * train_split)\n",
        "split_idx_test = int(images_tensor.size(0) * test_split)\n",
        "\n",
        "# Split the data\n",
        "train_features, test_features, val_features = shuffled_features[:split_idx_train], shuffled_features[split_idx_train:split_idx_test], shuffled_features[split_idx_test:]\n",
        "train_labels, test_labels, val_labels = shuffled_labels[:split_idx_train], shuffled_labels[split_idx_train:split_idx_test], shuffled_labels[split_idx_test:]\n",
        "\n",
        "# Create TensorDataset for training, test, and validation sets\n",
        "train_dataset = TensorDataset(train_features, train_labels)\n",
        "test_dataset = TensorDataset(test_features, test_labels)\n",
        "val_dataset = TensorDataset(val_features, val_labels)\n",
        "\n",
        "# Create DataLoaders with batching\n",
        "batch_size = 2\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converts YOLO grid to bounding boxes given a confidence threshold for bounding boxes to be considered\n",
        "def grid_to_bounding_boxes(grid, confidence_threshold=0.9, keep_confidence_score=False):\n",
        "    # Extract indices of bounding boxes\n",
        "    indices = torch.nonzero(grid[:, :, 4] > confidence_threshold)\n",
        "    bboxes = []\n",
        "    for idx in indices:\n",
        "        grid_x = idx[0]\n",
        "        grid_y = idx[1]\n",
        "        bbox = grid[grid_x, grid_y, :]\n",
        "\n",
        "        x_center = ((bbox[0] + grid_x) * GRID_WIDTH).item()\n",
        "        y_center = ((bbox[1]+ grid_y) * GRID_WIDTH).item()\n",
        "        x_width = (bbox[2] * IMAGE_SIZE).item()\n",
        "        y_width = (bbox[3] * IMAGE_SIZE).item()\n",
        "\n",
        "        # box = [x_center, y_center, x_width, y_width]\n",
        "        if keep_confidence_score:\n",
        "            box = [x_center - x_width / 2, y_center - y_width / 2, x_width, y_width, bbox[4].item()]\n",
        "        else:\n",
        "            box = [x_center - x_width / 2, y_center - y_width / 2, x_width, y_width]\n",
        "\n",
        "        bboxes.append(box)\n",
        "    return bboxes\n",
        "idx = -1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize images in the dataset (can be run repeatedly to see different examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx += 1\n",
        "\n",
        "i = shuffle_indices[idx].item()\n",
        "# i = idx  # Enable to view images in-order rather than shuffled\n",
        "\n",
        "image = images[i][0]\n",
        "img_name = img_names[i]\n",
        "bbox = grid_to_bounding_boxes(boxes[i])\n",
        "\n",
        "# Formats bounding boxes for drawing onto image plots\n",
        "def create_boxes(bounding_boxes):\n",
        "    boxes = []\n",
        "    for box in bounding_boxes:\n",
        "        boxes.append(Rectangle(box[0:2], box[2], box[3]))\n",
        "    return boxes\n",
        "\n",
        "# Display the image\n",
        "ax1 = plt.subplot()\n",
        "ax1.imshow(image, vmin=-1200, vmax=600, cmap='gray')\n",
        "print(img_name)\n",
        "# Display bounding boxes\n",
        "ax1.add_collection(PatchCollection(create_boxes(bbox), alpha = 0.25, facecolor = 'red'));\n",
        "# plt.imsave(img_names[idx], image, vmin=-1200, vmax=600, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxWmyM0ri1uW"
      },
      "outputs": [],
      "source": [
        "# Example of iterating through the train dataset\n",
        "print(len(train_loader))\n",
        "for images, bboxes in train_loader:\n",
        "    print(\"Train batch image shape:\", images.shape)\n",
        "    print(\"Train batch bounding boxes shape:\", bboxes.shape)\n",
        "    break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KcooPrTtwvy9"
      },
      "source": [
        "Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rse_0MArkMBa"
      },
      "outputs": [],
      "source": [
        "# Implemented after the YOLOv3 architecture diagram in https://arxiv.org/pdf/1804.02767\n",
        "class YoloModel(torch.nn.Module):\n",
        "    def __init__(self, grid_size=GRID_SIZE):\n",
        "        super(YoloModel, self).__init__()\n",
        "\n",
        "        self.transform = transform = transforms.Compose([\n",
        "            transforms.ToTensor(),  # Convert to PyTorch tensor (scales to [0, 1])\n",
        "            transforms.Lambda(lambda x: x - 32768),  # Subtract 32768\n",
        "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))  # Resize to desired dimensions\n",
        "        ])\n",
        "\n",
        "        self.grid_size = grid_size\n",
        "        \n",
        "        # Feature extraction layers\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # (batch, 16, 512, 512)\n",
        "            nn.ReLU(),  \n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # (batch, 64, 256, 256)\n",
        "            nn.ReLU(), \n",
        "            ResidualBlock(in_channels=32, repetitions=1),   \n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # (batch, 64, 128, 128)\n",
        "            nn.ReLU(), \n",
        "            ResidualBlock(in_channels=64, repetitions=2),                   \n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # (batch, 128, 64, 64)\n",
        "            nn.ReLU(), \n",
        "            ResidualBlock(in_channels=128, repetitions=4),                   \n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # (batch, 256, 32, 32)\n",
        "            nn.ReLU(),  \n",
        "            ResidualBlock(in_channels=256, repetitions=8),                   \n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),  # (batch, 512, 16, 16)\n",
        "            nn.ReLU(), \n",
        "            ResidualBlock(in_channels=512, repetitions=8),                   \n",
        "            nn.Conv2d(512, 1024, kernel_size=3, stride=2, padding=1),  # (batch, 1024, 8, 8)\n",
        "            nn.ReLU(), \n",
        "            ResidualBlock(in_channels=1024, repetitions=4)            \n",
        "        )\n",
        "        \n",
        "        # Prediction head\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.AvgPool2d(8),\n",
        "            nn.Flatten(start_dim=1),  # Resize to (batch, 1024) for linear layer\n",
        "            nn.Linear(1024, grid_size * grid_size * 5),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)  # Extract features\n",
        "        x = self.predictor(x)          # Predict grid-based output\n",
        "        x = x.view((-1, GRID_SIZE, GRID_SIZE, 5))  # Reshape to grid\n",
        "        return x\n",
        "    \n",
        "    def predict(self, img_path):\n",
        "        with Image.open(img_path).convert(\"F\") as im:\n",
        "            image = self.transform(im)\n",
        "        with torch.no_grad():\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            output = self(image.unsqueeze(0).to(device))\n",
        "            return output.squeeze(0)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, repetitions):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        layers = []\n",
        "        between_channels = in_channels // 2\n",
        "        for i in range(repetitions):\n",
        "            layers += [nn.Conv2d(in_channels, between_channels, kernel_size=1, stride=1, padding=0),\n",
        "                       nn.BatchNorm2d(between_channels),\n",
        "                       nn.ReLU(),\n",
        "                       nn.Conv2d(between_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
        "                       nn.BatchNorm2d(in_channels),\n",
        "                       nn.ReLU()]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # Save the input for the skip connection\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return F.relu(x + residual) # Add the input (skip connection)\n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SquareRootDistanceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SquareRootDistanceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            predictions: Tensor of predicted values (e.g., probabilities or weights)\n",
        "            targets: Tensor of ground truth values (same shape as predictions)\n",
        "        Returns:\n",
        "            loss: The calculated Square Root Distance Loss\n",
        "        \"\"\"\n",
        "        # Ensure all values are non-negative (to prevent NaNs in sqrt)\n",
        "        predictions = torch.clamp(predictions, min=0.0)\n",
        "        targets = torch.clamp(targets, min=0.0)\n",
        "\n",
        "        # Compute the square root of predictions and targets\n",
        "        sqrt_preds = torch.sqrt(predictions)\n",
        "        sqrt_targets = torch.sqrt(targets)\n",
        "\n",
        "        # Compute the squared difference\n",
        "        loss = ((sqrt_preds - sqrt_targets) ** 2).sum()\n",
        "        return loss\n",
        "\n",
        "class YOLOLoss(nn.Module):\n",
        "    def __init__(self, lambda_coord=1, lambda_noobj=0.1, confidence_threshold=0.5):\n",
        "        super(YOLOLoss, self).__init__()\n",
        "        self.ct = confidence_threshold    # Confidence required to make a positive prediction\n",
        "        self.lambda_coord = lambda_coord  # Weight for coordinate loss\n",
        "        self.lambda_noobj = lambda_noobj  # Weight for no-object loss\n",
        "        self.mse_loss = nn.MSELoss(reduction='sum')  # For coordinates and confidence\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='sum')  # For confidence scores\n",
        "        self.sqrd_loss = SquareRootDistanceLoss()\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            predictions: (batch_size, S, S, B*5 + C) - Predicted tensor\n",
        "            targets: (batch_size, S, S, B*5 + C) - Ground truth tensor\n",
        "\n",
        "        Components:\n",
        "            S: Grid size\n",
        "            B: Number of bounding boxes per grid cell\n",
        "            C: Number of classes\n",
        "        \"\"\"\n",
        "        obj_mask = targets[..., 4] >= self.ct  # Object mask (confidence >= ct)\n",
        "        noobj_mask = targets[..., 4] < self.ct  # No-object mask (confidence < ct)\n",
        "\n",
        "        # Coordinate Loss\n",
        "        coord_loss = self.mse_loss(\n",
        "            predictions[obj_mask][..., :2], targets[obj_mask][..., :2]\n",
        "        )\n",
        "\n",
        "        width_loss = self.sqrd_loss(\n",
        "            predictions[obj_mask][..., 2], targets[obj_mask][..., 2]\n",
        "        )\n",
        "\n",
        "        height_loss = self.sqrd_loss(\n",
        "            predictions[obj_mask][..., 3], targets[obj_mask][..., 3]\n",
        "        )\n",
        "\n",
        "        # Confidence Loss (Object)\n",
        "        obj_conf_loss = self.mse_loss(\n",
        "            predictions[obj_mask][..., 4], targets[obj_mask][..., 4]\n",
        "        )\n",
        "\n",
        "        # Confidence Loss (No Object)\n",
        "        noobj_conf_loss = self.mse_loss(\n",
        "            predictions[noobj_mask][..., 4], targets[noobj_mask][..., 4]\n",
        "        )\n",
        "\n",
        "        # Total Loss\n",
        "        total_loss = (\n",
        "            self.lambda_coord * (coord_loss + width_loss + height_loss)\n",
        "            + obj_conf_loss\n",
        "            + self.lambda_noobj * noobj_conf_loss\n",
        "        )\n",
        "\n",
        "        return total_loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if gpu support is enabled. If not, see https://pytorch.org/get-started/locally/ to install pytorch with CUDA support\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1pslRQFujnF"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = YoloModel(grid_size=GRID_SIZE).to(device)\n",
        "criterion = YOLOLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model (Skip if you have a pre-trained model) (can be executed multiple times to train more)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        # if (batch_idx + 1) % 50 == 0:  # Print every 50 batches\n",
        "        #     print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Get validation loss\n",
        "    model.eval()\n",
        "    running_val_loss = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "        # Move data to device\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Accumulate loss\n",
        "        running_val_loss += loss.item()\n",
        "\n",
        "    # Print epoch summary\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {running_loss / len(train_loader):.4f}, Validation Loss: {running_val_loss / len(val_loader):.4f}\")\n",
        "idx = -1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save model to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"model.pickle\", 'wb') as file:\n",
        "    dill.dump(model, file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load model from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"model.pickle\", 'rb') as file:\n",
        "    model = dill.load(file)\n",
        "idx = -1\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAz_NYEIw3XA"
      },
      "source": [
        "Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "def get_acc(data_loader, min_conf=0.5):\n",
        "    total_acc = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "        # Move data to device\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        obj_mask_t = (targets[..., 4] >= min_conf).nonzero()\n",
        "        obj_mask_y = (outputs[..., 4] >= min_conf).nonzero()\n",
        "\n",
        "        # Measure accuracy as % of positive predictions which match between the labels and predictions\n",
        "        total = torch.cat((obj_mask_t, obj_mask_y))\n",
        "        union = total.unique(dim=0)\n",
        "        len_intersect = len(total) - len(union)\n",
        "        acc = len_intersect / len(total)\n",
        "\n",
        "        total_acc += acc\n",
        "    return total_acc / (batch_idx + 1)\n",
        "\n",
        "print(f\"Average accuracy on training set: {get_acc(train_loader):.4f}\")\n",
        "print(f\"Average accuracy on testing set: {get_acc(test_loader):.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize model outputs (can be run repeatedly to see different examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5CduXbtumx4"
      },
      "outputs": [],
      "source": [
        "# idx = 3\n",
        "idx += 1\n",
        "\n",
        "image, label_bbox = train_dataset[idx]\n",
        "\n",
        "# Plot two images for comparison\n",
        "figs, axes = plt.subplots(1, 2)\n",
        "ax1 = axes[0]\n",
        "ax2 = axes[1]\n",
        "ax1.imshow(image[0], vmin=-1200, vmax=600, cmap='gray')\n",
        "ax2.imshow(image[0], vmin=-1200, vmax=600, cmap='gray')\n",
        "grid = model(image[np.newaxis, :].to(device))[0, :]\n",
        "# Print image name and maximum confidence score within the grids\n",
        "print(f\"{img_name} - Max confidence: {grid[:, :, 4].max().item():.3f}\")\n",
        "\n",
        "alpha_confidence = False  # Displays bounding box transparency by confidence score\n",
        "# alpha_confidence = True\n",
        "# 0.5 is the min confidence threshold for bounding boxes to be visualized\n",
        "bbox = grid_to_bounding_boxes(grid, 0.5, alpha_confidence)\n",
        "# Print bounding box dimensions (x_center, y_center, width, height)\n",
        "print(\"Bounding boxes:\", bbox)\n",
        "\n",
        "# Plot bounding boxes\n",
        "if alpha_confidence:\n",
        "    print([x[4] for x in bbox])\n",
        "    ax1.add_collection(PatchCollection(create_boxes(bbox), alpha = [x[4] for x in bbox], facecolor = 'red'))\n",
        "else:\n",
        "    ax1.add_collection(PatchCollection(create_boxes(bbox), alpha = 0.25, facecolor = 'red'))\n",
        "ax2.add_collection(PatchCollection(create_boxes(grid_to_bounding_boxes(label_bbox)), alpha = 0.25, facecolor = 'red'));"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
